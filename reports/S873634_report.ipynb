{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Learning in Stochastic Bandits\n",
        "\n",
        "| | |\n",
        "| --- | --- |\n",
        "| Problem | Learning user preferences online might have an impact of delay and training recommender system sequentially for every example is computationally heavy. |\n",
        "| Hypothesis | A learning agent observes responses batched in groups over a certain time period. The impact of batch learning can be measured in terms of online behavior. |\n",
        "| Prblm Stmt. | Given a finite set of arms ⁍, an environment ⁍ (⁍ is the distribution of rewards for action ⁍), and a time horizon ⁍, at each time step ⁍, the agent chooses an action ⁍ and receives a reward ⁍. The goal of the agent is to maximize the total reward ⁍. |\n",
        "| Solution | Sequential batch learning is a more generalized way of learning which covers both offline and online settings as special cases bringing together their advantages. Unlike offline learning, sequential batch learning retains the sequential nature of the problem. Unlike online learning, it is often appealing to implement batch learning in large scale bandit problems. In this setting, responses are grouped in batches and observed by the agent only at the end of each batch. |\n",
        "| Dataset | Mushroom, Synthetic |\n",
        "| Preprocessing | Train/test split, label encoding |\n",
        "| Metrics | Conversion rate, regret |\n",
        "| Credits | Danil Provodin |\n",
        "\n",
        "## Model\n",
        "\n",
        "### Environments\n",
        "\n",
        "| Name | Type | Rewards |\n",
        "| --- | --- | --- |\n",
        "| env1 | 2-arm environment | [0.7, 0.5] |\n",
        "| env2 | 2-arm environment | [0.7, 0.4] |\n",
        "| env3 | 2-arm environment | [0.7, 0.1] |\n",
        "| env4 | 4-arm environment | [0.35, 0.18, 0.47, 0.61] |\n",
        "| env5 | 4-arm environment | [0.40, 0.75, 0.57, 0.49] |\n",
        "| env6 | 4-arm environment | [0.70, 0.50, 0.30, 0.10] |\n",
        "\n",
        "### Simulation\n",
        "\n",
        "| Application | Policy |\n",
        "| --- | --- |\n",
        "| Multi-armed bandit (MAB) | Thompson Sampling (TS) |\n",
        "| Multi-armed bandit (MAB) | Upper Confidence Bound (UCB) |\n",
        "| Contextual MAB (CMAB) | Linear Thompson Sampling (LinTS) |\n",
        "| Contextual MAB (CMAB) | Linear UCB (LinUCB) |\n",
        "\n",
        "## Tutorials\n",
        "\n",
        "### Sequential Batch Learning in Stochastic MAB and Contextual MAB on Mushroom and Synthetic data\n",
        "\n",
        "[direct link to notebook →](https://github.com/RecoHut-Stanzas/S873634/blob/main/nbs/P296669_Sequential_Batch_Learning_in_Stochastic_MAB_and_Contextual_MAB_on_Mushroom_and_Synthetic_data.ipynb)\n",
        "\n",
        "![https://github.com/RecoHut-Stanzas/S873634/raw/main/images/process_flow.svg](https://github.com/RecoHut-Stanzas/S873634/raw/main/images/process_flow.svg)\n",
        "\n",
        "## References\n",
        "\n",
        "1. [https://github.com/RecoHut-Stanzas/S873634](https://github.com/RecoHut-Stanzas/S873634)\n",
        "2. [https://arxiv.org/abs/2111.02071v1](https://arxiv.org/abs/2111.02071v1)\n",
        "3. [https://github.com/danilprov/batch-bandits](https://github.com/danilprov/batch-bandits)"
      ],
      "metadata": {
        "id": "lIYdn1woOS1n"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}